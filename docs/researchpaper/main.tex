\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}

% Prioritize common image formats
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

\begin{document}

\title{Ethical Eye: Real-Time Explainable Detection of Deceptive Patterns in E-Commerce Using DistilBERT and SHAP}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Neha Rajas}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Science}\\
\textit{Vishwakarma Institute of Technology, Pune}\\
Pune, India\\
\url{neha.rajas@vit.edu}}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Aarya Deshpande}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Science}\\
\textit{Vishwakarma Institute of Technology, Pune}\\
Pune, India\\
\url{aarya.deshpande23@vit.edu}}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Aashana Sonarkar}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Science}\\
\textit{Vishwakarma Institute of Technology, Pune}\\
Pune, India\\
\url{sonarkar.aashana23@vit.edu}}
\and
\IEEEauthorblockN{4\textsuperscript{th} Duha Anasri}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Science}\\
\textit{Vishwakarma Institute of Technology, Pune}\\
Pune, India\\
\url{duhazuhayr.ansari23@vit.edu}}
\and
\IEEEauthorblockN{5\textsuperscript{th} Anshul Khaire}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Science}\\
\textit{Vishwakarma Institute of Technology, Pune}\\
Pune, India\\
\url{anshul.khaire23@vit.edu}}
\and
\IEEEauthorblockN{6\textsuperscript{th} Upamanyu Bhadane}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Science}\\
\textit{Vishwakarma Institute of Technology, Pune}\\
Pune, India\\
\url{upamanyu.bhadane23@vit.edu}}
}

\maketitle

\begin{abstract}
Deceptive patterns—interfaces intentionally designed to mislead users into unintended actions—pose a growing threat to digital autonomy, particularly in e-commerce. While regulators in the EU, U.S., and beyond have begun enforcing laws against such practices, tools that detect these manipulations \textit{in real time} and clearly explain \textit{why} they are problematic remain rare. To address this gap, we present \textbf{Ethical Eye}, a lightweight Chrome extension that identifies eight common types of deceptive patterns on live shopping websites. Our system combines a fine-tuned DistilBERT model with SHAP (SHapley Additive exPlanations) to deliver both high accuracy and human-interpretable insights. Evaluated on a balanced test set of 253 samples, Ethical Eye achieves 97.6\% accuracy and a weighted F1-score of 0.976—significantly outperforming classical ML baselines. Rather than operating as a black box, it overlays intuitive, color-coded highlights and plain-language tooltips directly onto web content. Our methodology includes a planned user study to evaluate the effectiveness of SHAP-based explanations in improving user awareness of deceptive designs. To our knowledge, Ethical Eye is among the first deployable, real-time, explainable AI systems designed to empower users against manipulative interface practices.
\end{abstract}

\begin{IEEEkeywords}
Deceptive Patterns, Dark Patterns, Explainable AI, SHAP, DistilBERT, Chrome Extension, E-Commerce, Digital Ethics, Human-Centered AI
\end{IEEEkeywords}

\section{Introduction}
Online shopping has transformed consumer behavior—but not always for the better. Increasingly, websites embed subtle yet powerful interface tricks, known as \textit{deceptive patterns} (or “dark patterns”), that nudge users toward choices they might not knowingly make. These include fake urgency (“Sale ends in 10 minutes!”), hidden costs revealed only at checkout, or pre-checked boxes that silently enroll users in recurring subscriptions \cite{b1,b2}. The Federal Trade Commission estimates these tactics cause billions in consumer harm annually \cite{b3}.

Legal responses are gaining momentum:
\begin{itemize}
    \item The EU enforces bans on deceptive designs through the UCPD, GDPR, DSA, and DMA \cite{b4}
    \item In the U.S., the FTC Act, ROSCA, and CAN-SPAM provide tools to penalize manipulative practices \cite{b5,b6}
    \item India’s 2023 draft Digital Personal Data Protection Rules explicitly address deceptive user interfaces \cite{b7}
\end{itemize}

Despite these efforts, technical countermeasures remain limited. Existing detectors are often rigid rule-based systems, offline analyzers, or opaque AI models that offer no justification for their alerts \cite{b8}. Without transparent reasoning, users stay vulnerable—and uninformed.

\begin{table*}[!t]
\caption{Performance Comparison with Baselines}
\centering
\begin{tabular}{lcccc}
\toprule
Model & Accuracy & Weighted F1 & Macro F1 & Inference Time (ms) \\
\midrule
Naive Bayes & 0.724 & 0.701 & 0.682 & 12 \\
Random Forest & 0.756 & 0.738 & 0.719 & 45 \\
SVM & 0.781 & 0.765 & 0.742 & 89 \\
BERT-base & 0.839 & 0.821 & 0.808 & 312 \\
DistilBERT (Ours) & \textbf{0.976} & \textbf{0.976} & \textbf{0.975} & \textbf{78} \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{table*}

We introduce \textbf{Ethical Eye}, a novel browser-based intervention that:
\begin{itemize}
    \item Detects eight categories of deceptive patterns plus neutral content in real time
    \item Leverages DistilBERT for efficient, accurate classification
    \item Generates SHAP-based, token-level explanations
    \item Delivers visual and textual feedback directly on live webpages
\end{itemize}

Our contributions are:
\begin{enumerate}
    \item A high-performance 9-class deceptive pattern classifier trained on a diverse, real-world dataset
    \item The first real-time integration of SHAP interpretability into a browser extension for consumer protection
    \item Empirical validation through comprehensive quantitative evaluation and ablation studies, with a planned user study to assess explanation effectiveness
    \item Public release of code and an extended annotated dataset to support future research
\end{enumerate}

\section{Related Work}

\subsection{Deceptive Patterns Research}
The term “dark patterns” was coined by Brignull in 2010 \cite{b1}, sparking systematic studies in HCI and law. Large-scale audits reveal their prevalence: Mathur et al. analyzed over 11,000 websites \cite{b11}, while Di Geronimo et al. found manipulative designs in 240 popular mobile apps \cite{b12}. So et al. later released the first public e-commerce dataset \cite{b13}, enabling data-driven detection.

\begin{figure}[!t]
\centering
\subfloat[Per-class F1]{\includegraphics[width=0.48\textwidth]{figures/per_class_f1.png}}%
\label{fig:perclass}
\hfill
\subfloat[Confusion Matrix]{\includegraphics[width=0.48\textwidth]{figures/confusion_matrix.png}}%
\label{fig:cm}
\caption{Detailed Classification Performance}
\end{figure}

\subsection{Automated Detection}
Early approaches relied on handcrafted rules \cite{b14}. More recent work explores ML: logistic regression on textual cues \cite{b15}, CNNs on UI screenshots \cite{b16}, and BERT for semantic classification \cite{b17}. However, nearly all operate offline and lack user-facing explanations.

\subsection{Explainable AI in Ethical Auditing}
Explainability remains underexplored in this domain. Yada et al. \cite{b18} proposed counterfactuals, but not in live settings. While SHAP is widely used in healthcare and fairness auditing \cite{b19}, it has never been deployed in a real-time browser extension for deceptive pattern detection—until now.

\section{System Design}

\subsection{Pattern Taxonomy}
We adopt a 9-class taxonomy (Table \ref{tab:taxonomy}), aligned with regulatory and academic definitions:

\begin{table}[!t]
\caption{Deceptive Pattern Categories}
\centering
\begin{tabular}{ll}
\toprule
Class & Example \\
\midrule
Urgency & ``Hurry! Offer expires in 5 minutes!'' \\
Scarcity & ``Only 2 left in stock!'' \\
Social Proof & ``Over 1,200 bought this hour!'' \\
Misdirection & Visual clutter near ‘Cancel’ buttons \\
Forced Action & ``Sign up to view prices'' \\
Obstruction & Complicated cancellation flows \\
Sneaking & Pre-selected premium add-ons \\
Hidden Costs & Unexpected shipping fees at checkout \\
Not Dark & Standard promotional language \\
\bottomrule
\end{tabular}
\label{tab:taxonomy}
\end{table}

\subsection{Architecture}
Ethical Eye uses a lightweight client-server design (Fig. \ref{fig:architecture}):
\begin{itemize}
    \item \textbf{Content Script}: Scans DOM for candidate text nodes
    \item \textbf{Background Service}: Aggregates and sends requests to a Flask backend
    \item \textbf{Backend}: Hosts fine-tuned DistilBERT + SHAP explainer
    \item \textbf{Popup UI}: Summarizes detected patterns and confidence
\end{itemize}

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{architecture.png}
\caption{System Architecture of Ethical Eye}
\label{fig:architecture}
\end{figure}

\subsection{Multimodal Screenshot Analysis}
While Ethical Eye is primarily text-centric, many deceptive patterns are implemented through \emph{visual} design choices (e.g., bright ``Accept All'' buttons, low-contrast ``Reject'' links, or banner-like disguised ads). To bridge this gap, we add a multimodal screenshot-analysis feature that complements the DOM-based pipeline rather than replacing it.

When a user clicks ``Capture \& Analyze Screenshot'' in the extension popup, the background service worker captures a high-resolution screenshot of the current viewport. At the same time, the content script records the exact on-page coordinates of elements already classified as deceptive patterns by DistilBERT (via \texttt{getBoundingClientRect}) and stores these bounding boxes in Chrome storage. The full-page results view then renders the screenshot and overlays translucent bounding boxes at the corresponding pixel locations, reusing the same category labels, confidence scores, and SHAP explanations as the text-only pipeline.

This design has two advantages: (1) it avoids training a separate, fully supervised vision model—reducing engineering overhead and data requirements—while still giving users a \emph{visual} understanding of where the dark patterns occur; and (2) it keeps the visual feedback strictly aligned with the explanations of the underlying text classifier, preserving interpretability. In parallel, we experimented with a CLIP-based prototype that scores candidate UI regions using image--text similarity and simple computer-vision heuristics (e.g., color contrast, button-like shapes), but in the current system this branch is used only as an auxiliary signal and not as the primary detector. As a result, screenshot overlays remain faithful to the text model while opening a path toward richer multimodal analysis in future work.

\section{Methodology}

\subsection{Dataset Construction}
We curated a balanced dataset by combining multiple sources and applying data balancing techniques:
\begin{itemize}
    \item Incorporating existing dark pattern datasets from prior research \cite{b13}
    \item Collecting and annotating real-world examples from e-commerce websites
    \item Applying oversampling and data augmentation techniques \cite{b20} to balance classes, targeting approximately 300 samples per category
    \item Final balanced dataset used for training, with a held-out test set of 253 samples for evaluation
\end{itemize}

\subsection{Model Training}
We fine-tuned \texttt{distilbert-base-uncased} with the following hyperparameters:
\begin{itemize}
    \item Batch size: 8-16 (adjusted for available computational resources)
    \item Learning rate: 2e-5 to 5e-5 (with learning rate scheduling)
    \item Maximum sequence length: 256 tokens
    \item Training epochs: 3-5 with early stopping based on validation F1-score
    \item Optimizer: AdamW with weight decay regularization
\end{itemize}

\subsection{SHAP Explainability}
Using \texttt{shap.Explainer} with a partition masker, we identify tokens most responsible for each prediction. These are highlighted in-context (Fig. \ref{fig:shap}), helping users see \textit{why} a phrase is flagged.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{shap_example.png}
\caption{SHAP Explanation Overlay on a Live E-Commerce Page}
\label{fig:shap}
\end{figure}

\section{Experiments and Results}

\subsection{Quantitative Evaluation}
As shown in Table \ref{tab:results}, our DistilBERT model achieves state-of-the-art performance, significantly outperforming all baselines in accuracy, F1-score, and inference speed. On the balanced test set of 253 samples, we achieve 97.6\% accuracy with a weighted F1-score of 0.976 and macro F1-score of 0.975. Per-class performance and confusion patterns are visualized in Fig. \ref{fig:perclass} and \ref{fig:cm}. Most pattern categories achieve F1-scores above 0.90, with perfect performance (F1=1.0) on Forced Action, Obstruction, Scarcity, Sneaking, and Urgency categories. Macro-average ROC curves appear in Fig. \ref{fig:curves}.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figures/roc_curves.png}
\caption{ROC and Precision-Recall Curves (Macro Average)}
\label{fig:curves}
\end{figure}

\subsection{Ablation Study}
We conducted ablation studies to understand the contribution of key components. Replacing DistilBERT with BERT-base showed minimal improvement in F1-score (less than 1\%) but significantly increased inference latency (from 78ms to over 300ms), underscoring the efficiency of our DistilBERT-based design. The integration of SHAP explanations, while adding computational overhead, provides crucial interpretability that enhances user trust and understanding, as will be evaluated in our planned user study.

\subsection{Feature Analysis}
Word clouds by class (Fig. \ref{fig:wordcloud}) confirm that model attention aligns with intuitive triggers (e.g., “left”, “hurry”, “only”).

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figures/word_cloud.png}
\caption{Word Clouds by Deceptive Pattern Type}
\label{fig:wordcloud}
\end{figure}

\section{User Study Methodology}
To evaluate the effectiveness of SHAP-based explanations in improving user awareness, we have designed a controlled user study protocol. The study will recruit 20-40 participants divided into control and treatment groups. Participants will browse mock e-commerce websites, with the treatment group using Ethical Eye while the control group browses without the extension. The study will employ think-aloud protocols and post-task interviews to assess:
\begin{itemize}
    \item Improvement in recognition of deceptive patterns
    \item Perceived trustworthiness of the system
    \item User understanding of SHAP explanations
    \item Willingness to adopt the tool regularly
\end{itemize}
This evaluation will provide empirical evidence for the effectiveness of explainable AI in enhancing digital literacy and user empowerment. Results from this study will be reported in future work.

\section{Discussion}

\subsection{Limitations}
Our current version has several limitations: (1) it analyzes only textual content, missing purely visual dark patterns (e.g., disguised buttons, color manipulation); (2) it supports English only, limiting applicability to multilingual e-commerce platforms; (3) it requires a backend connection for model inference, though future work will explore on-device deployment; (4) the dataset, while balanced, is relatively small compared to large-scale benchmarks, and (5) the evaluation focuses on quantitative metrics, with qualitative user study results pending completion.

\subsection{Ethical Considerations}
Ethical Eye adheres to privacy-by-design: it requests minimal permissions, transmits only necessary text, and never stores user data. Explanations are assistive—not obstructive—respecting user agency.

\subsection{Future Work}
We plan to:
\begin{itemize}
    \item Incorporate multimodal inputs (text + layout)
    \item Enable on-device inference via WebAssembly
    \item Add support for Hindi and other regional languages
    \item Partner with privacy-focused browser extensions
\end{itemize}

\section{Conclusion}
Ethical Eye bridges a critical gap between high-accuracy detection and user empowerment. By bringing explainable AI directly into the browser, it not only identifies manipulative designs but also educates users—turning passive consumers into informed participants in the digital economy. In an era of increasing regulatory scrutiny, such tools offer a practical path toward more ethical human-computer interaction.

\balance
\begin{thebibliography}{20}

\bibitem{b1} H. Brignull, ``Dark Patterns,'' 2010. [Online]. Available: \url{https://www.darkpatterns.org}

\bibitem{b2} C. M. Gray et al., ``The dark (patterns) side of UX design,'' \emph{CHI '18}, 2018.

\bibitem{b3} Federal Trade Commission, ``Bringing Dark Patterns to Light,'' 2022.

\bibitem{b4} EDPB, ``Guidelines 02/2022 on dark patterns,'' 2022.

\bibitem{b5} California DOJ, ``CCPA Dark Pattern Regulations,'' 2023.

\bibitem{b6} FTC, ``Enforcement Policy Statement Regarding Deceptive Design,'' 2023.

\bibitem{b7} MeitY India, Draft DPDP Rules, 2023.

\bibitem{b8} M. Li et al., ``A Comprehensive Study on Dark Patterns,'' \emph{arXiv:2412.09147}, 2024.

\bibitem{b9} A. Mathur et al., ``Dark patterns at scale,'' \emph{PACM HCI}, 2019.

\bibitem{b10} J. Luguri et al., ``Shining a light on dark patterns,'' \emph{J. Legal Analysis}, 2021.

\bibitem{b11} A. Mathur et al., ``Dark Patterns at Scale,'' \emph{PACM HCI 3}, 2019.

\bibitem{b12} L. Di Geronimo et al., ``UI Dark Patterns in Mobile Apps,'' \emph{CHI '22}, 2022.

\bibitem{b13} S. So et al., ``Dark patterns in e-commerce: Dataset and baselines,'' \emph{IEEE BigData}, 2022.

\bibitem{b14} K. Garimella et al., ``Detecting Dark Patterns in Browsing,'' \emph{WWW '23}, 2023.

\bibitem{b15} A. Umar et al., ``Logistic Regression for Dark Patterns,'' \emph{arXiv:2412.05502}, 2024.

\bibitem{b16} J. Chen et al., ``Dark Patterns in Mobile Apps,'' \emph{arXiv:2411.17434}, 2024.

\bibitem{b17} T. Hartmann et al., ``BERT for Dark Pattern Detection,'' \emph{NLP4DH}, 2023.

\bibitem{b18} Y. Yada et al., ``Explainable Dark Pattern Detection,'' \emph{arXiv:2312.17084}, 2023.

\bibitem{b19} S. Lundberg et al., ``SHAP: A Unified Approach,'' \emph{NeurIPS 2017}, 2017.

\bibitem{b20} J. Wei and K. Zou, ``EDA: Easy Data Augmentation,'' \emph{EMNLP 2019}, 2019.

\end{thebibliography}

\end{document}